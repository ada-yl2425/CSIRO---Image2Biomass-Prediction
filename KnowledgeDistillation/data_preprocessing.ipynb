{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c45e06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T20:44:59.168535Z",
     "iopub.status.busy": "2025-11-09T20:44:59.168230Z",
     "iopub.status.idle": "2025-11-09T20:45:03.025124Z",
     "shell.execute_reply": "2025-11-09T20:45:03.024087Z"
    },
    "papermill": {
     "duration": 3.862594,
     "end_time": "2025-11-09T20:45:03.027084",
     "exception": false,
     "start_time": "2025-11-09T20:44:59.164490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pickle  # 用于保存 'fitter' 对象\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a96bec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T20:45:03.034499Z",
     "iopub.status.busy": "2025-11-09T20:45:03.034015Z",
     "iopub.status.idle": "2025-11-09T20:45:03.041202Z",
     "shell.execute_reply": "2025-11-09T20:45:03.040042Z"
    },
    "papermill": {
     "duration": 0.012608,
     "end_time": "2025-11-09T20:45:03.043070",
     "exception": false,
     "start_time": "2025-11-09T20:45:03.030462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 1. 配置 ---\n",
    "\n",
    "# 原始数据文件\n",
    "RAW_TRAIN_CSV = '/kaggle/input/csiro-biomass/train.csv'\n",
    "\n",
    "# 处理后数据的输出路径\n",
    "OUTPUT_DIR = \"/kaggle/working/\"\n",
    "PROCESSED_TRAIN_CSV = os.path.join(OUTPUT_DIR, 'train_processed.csv')\n",
    "SCALER_PATH = os.path.join(OUTPUT_DIR, 'scaler.pkl')\n",
    "ENCODERS_PATH = os.path.join(OUTPUT_DIR, 'encoders.pkl')\n",
    "\n",
    "# 定义列组\n",
    "TARGET_COLS = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']\n",
    "NUM_FEATURES = ['Pre_GSHH_NDVI', 'Height_Ave_cm']\n",
    "CAT_FEATURES = ['State', 'Species']\n",
    "TIME_FEATURE = 'Sampling_Date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44a15b01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T20:45:03.048213Z",
     "iopub.status.busy": "2025-11-09T20:45:03.047771Z",
     "iopub.status.idle": "2025-11-09T20:45:03.059180Z",
     "shell.execute_reply": "2025-11-09T20:45:03.058159Z"
    },
    "papermill": {
     "duration": 0.016079,
     "end_time": "2025-11-09T20:45:03.060840",
     "exception": false,
     "start_time": "2025-11-09T20:45:03.044761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 2. 核心处理函数 ---\n",
    "\n",
    "def process_dataframe(csv_path, is_train=True, scaler=None, encoders=None):\n",
    "    \"\"\"\n",
    "    加载并处理整个数据集（训练或测试）。\n",
    "    \n",
    "    is_train=True: \n",
    "        - 会 fit_transform 变换器\n",
    "        - 会处理目标列\n",
    "        - 会返回 (df, scaler, encoders)\n",
    "    is_train=False: \n",
    "        - 会使用传入的 scaler/encoders 进行 transform\n",
    "        - 不会处理目标列\n",
    "        - 会返回 (df)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- A. 加载与重塑 ---\n",
    "    df_long = pd.read_csv(csv_path)\n",
    "    \n",
    "    # 修正后的重塑逻辑 (使用 image_path 作为唯一ID)\n",
    "    df_wide_targets = df_long.pivot(index='image_path', columns='target_name', values='target')\n",
    "    \n",
    "    meta_cols = ['image_path', TIME_FEATURE] + CAT_FEATURES + NUM_FEATURES\n",
    "    available_meta_cols = [col for col in meta_cols if col in df_long.columns]\n",
    "    df_meta = df_long[available_meta_cols].drop_duplicates(subset='image_path').set_index('image_path')\n",
    "    \n",
    "    df_wide = df_meta.join(df_wide_targets)\n",
    "    \n",
    "    # 复制一份用于输出，避免 SettingWithCopyWarning\n",
    "    df_out = df_wide.copy()\n",
    "\n",
    "    # --- B. 处理目标 (Y) - 仅限训练集 ---\n",
    "    if is_train:\n",
    "        print(\"Processing targets (Log Transform)...\")\n",
    "        for col in TARGET_COLS:\n",
    "            df_out[f'log_{col}'] = np.log1p(df_out[col])\n",
    "    \n",
    "    # --- C. 处理时间特征 (X_time) ---\n",
    "    print(\"Processing time features (Month sin/cos)...\")\n",
    "    df_out[TIME_FEATURE] = pd.to_datetime(df_out[TIME_FEATURE])\n",
    "    df_out['Month'] = df_out[TIME_FEATURE].dt.month\n",
    "    df_out['month_sin'] = np.sin(2 * np.pi * df_out['Month'] / 12)\n",
    "    df_out['month_cos'] = np.cos(2 * np.pi * df_out['Month'] / 12)\n",
    "    \n",
    "    # --- D. 处理数值特征 (X_num) ---\n",
    "    print(\"Processing numeric features (StandardScaler)...\")\n",
    "    if is_train:\n",
    "        # 如果是训练，创建并 fit 新的 scaler\n",
    "        scaler = StandardScaler()\n",
    "        df_out[NUM_FEATURES] = scaler.fit_transform(df_out[NUM_FEATURES])\n",
    "    else:\n",
    "        # 如果是测试，使用之前 fit 好的 scaler\n",
    "        if scaler is None:\n",
    "            raise ValueError(\"Scaler must be provided for test data processing\")\n",
    "        df_out[NUM_FEATURES] = scaler.transform(df_out[NUM_FEATURES])\n",
    "\n",
    "    # --- E. 处理类别特征 (X_cat) ---\n",
    "    print(\"Processing categorical features (LabelEncoder)...\")\n",
    "    if is_train:\n",
    "        # 如果是训练，创建并 fit 新的 encoders\n",
    "        encoders = {}\n",
    "        for col in CAT_FEATURES:\n",
    "            le = LabelEncoder()\n",
    "            df_out[f'{col}_encoded'] = le.fit_transform(df_out[col])\n",
    "            encoders[col] = le\n",
    "    else:\n",
    "        # 如果是测试，使用之前 fit 好的 encoders\n",
    "        if encoders is None:\n",
    "            raise ValueError(\"Encoders must be provided for test data processing\")\n",
    "        for col in CAT_FEATURES:\n",
    "            # 使用 .transform()。如果测试集出现训练集没有的标签，会报错\n",
    "            df_out[f'{col}_encoded'] = encoders[col].transform(df_out[col])\n",
    "\n",
    "    # --- F. 返回结果 ---\n",
    "    if is_train:\n",
    "        return df_out, scaler, encoders\n",
    "    else:\n",
    "        # 确保测试集的列顺序与训练集一致\n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39adb4aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T20:45:03.065990Z",
     "iopub.status.busy": "2025-11-09T20:45:03.065495Z",
     "iopub.status.idle": "2025-11-09T20:45:03.190621Z",
     "shell.execute_reply": "2025-11-09T20:45:03.189429Z"
    },
    "papermill": {
     "duration": 0.129795,
     "end_time": "2025-11-09T20:45:03.192387",
     "exception": false,
     "start_time": "2025-11-09T20:45:03.062592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Training Data ---\n",
      "Processing targets (Log Transform)...\n",
      "Processing time features (Month sin/cos)...\n",
      "Processing numeric features (StandardScaler)...\n",
      "Processing categorical features (LabelEncoder)...\n",
      "\n",
      "Processed training data shape: (357, 20)\n",
      "Saving processed train CSV to: /kaggle/working/train_processed.csv\n",
      "Saving scaler to: /kaggle/working/scaler.pkl\n",
      "Saving encoders to: /kaggle/working/encoders.pkl\n",
      "\n",
      "--- Data Processing Pipeline COMPLETED ---\n",
      "\n",
      "Processed Data Head:\n",
      "                       Sampling_Date State            Species  Pre_GSHH_NDVI  \\\n",
      "image_path                                                                     \n",
      "train/ID1011485656.jpg    2015-09-04   Tas    Ryegrass_Clover      -0.246319   \n",
      "train/ID1012260530.jpg    2015-04-01   NSW            Lucerne      -0.707060   \n",
      "train/ID1025234388.jpg    2015-09-01    WA  SubcloverDalkeith      -1.826004   \n",
      "train/ID1028611175.jpg    2015-05-18   Tas           Ryegrass       0.016962   \n",
      "train/ID1035947949.jpg    2015-09-11   Tas           Ryegrass      -0.772880   \n",
      "\n",
      "                        Height_Ave_cm  Dry_Clover_g  Dry_Dead_g  Dry_Green_g  \\\n",
      "image_path                                                                     \n",
      "train/ID1011485656.jpg      -0.285204        0.0000     31.9984      16.2751   \n",
      "train/ID1012260530.jpg       0.818240        0.0000      0.0000       7.6000   \n",
      "train/ID1025234388.jpg      -0.642205        6.0500      0.0000       0.0000   \n",
      "train/ID1028611175.jpg      -0.252753        0.0000     30.9703      24.2376   \n",
      "train/ID1035947949.jpg      -0.398797        0.4343     23.2239      10.5261   \n",
      "\n",
      "                        Dry_Total_g    GDM_g  log_Dry_Green_g  log_Dry_Dead_g  \\\n",
      "image_path                                                                      \n",
      "train/ID1011485656.jpg      48.2735  16.2750         2.849266        3.496459   \n",
      "train/ID1012260530.jpg       7.6000   7.6000         2.151762        0.000000   \n",
      "train/ID1025234388.jpg       6.0500   6.0500         0.000000        0.000000   \n",
      "train/ID1028611175.jpg      55.2079  24.2376         3.228335        3.464807   \n",
      "train/ID1035947949.jpg      34.1844  10.9605         2.444614        3.187340   \n",
      "\n",
      "                        log_Dry_Clover_g  log_GDM_g  log_Dry_Total_g  Month  \\\n",
      "image_path                                                                    \n",
      "train/ID1011485656.jpg          0.000000   2.849260         3.897386      9   \n",
      "train/ID1012260530.jpg          0.000000   2.151762         2.151762      4   \n",
      "train/ID1025234388.jpg          1.953028   1.953028         1.953028      9   \n",
      "train/ID1028611175.jpg          0.000000   3.228335         4.029057      5   \n",
      "train/ID1035947949.jpg          0.360677   2.481610         3.560603      9   \n",
      "\n",
      "                        month_sin     month_cos  State_encoded  \\\n",
      "image_path                                                       \n",
      "train/ID1011485656.jpg  -1.000000 -1.836970e-16              1   \n",
      "train/ID1012260530.jpg   0.866025 -5.000000e-01              0   \n",
      "train/ID1025234388.jpg  -1.000000 -1.836970e-16              3   \n",
      "train/ID1028611175.jpg   0.500000 -8.660254e-01              1   \n",
      "train/ID1035947949.jpg  -1.000000 -1.836970e-16              1   \n",
      "\n",
      "                        Species_encoded  \n",
      "image_path                               \n",
      "train/ID1011485656.jpg               11  \n",
      "train/ID1012260530.jpg                3  \n",
      "train/ID1025234388.jpg               12  \n",
      "train/ID1028611175.jpg               10  \n",
      "train/ID1035947949.jpg               10  \n"
     ]
    }
   ],
   "source": [
    "# --- 3. 执行管道 ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # 确保输出目录存在\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        \n",
    "        # --- 处理训练数据 ---\n",
    "        print(\"--- Processing Training Data ---\")\n",
    "        train_df_processed, fitted_scaler, fitted_encoders = process_dataframe(\n",
    "            RAW_TRAIN_CSV, \n",
    "            is_train=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nProcessed training data shape: {train_df_processed.shape}\")\n",
    "        \n",
    "        # --- 保存输出 ---\n",
    "        print(f\"Saving processed train CSV to: {PROCESSED_TRAIN_CSV}\")\n",
    "        train_df_processed.to_csv(PROCESSED_TRAIN_CSV, index=True) # index=True 保存 image_path\n",
    "        \n",
    "        print(f\"Saving scaler to: {SCALER_PATH}\")\n",
    "        with open(SCALER_PATH, 'wb') as f:\n",
    "            pickle.dump(fitted_scaler, f)\n",
    "            \n",
    "        print(f\"Saving encoders to: {ENCODERS_PATH}\")\n",
    "        with open(ENCODERS_PATH, 'wb') as f:\n",
    "            pickle.dump(fitted_encoders, f)\n",
    "            \n",
    "        print(\"\\n--- Data Processing Pipeline COMPLETED ---\")\n",
    "        \n",
    "        print(\"\\nProcessed Data Head:\")\n",
    "        print(train_df_processed.head())\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Raw data file not found at {RAW_TRAIN_CSV}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.965229,
   "end_time": "2025-11-09T20:45:03.916983",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-09T20:44:53.951754",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
