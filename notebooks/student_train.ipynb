{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":4608.36957,"end_time":"2025-11-28T12:33:07.850497","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-11-28T11:16:19.480927","version":"2.6.0"},"vscode":{"interpreter":{"hash":"d1577353fdb758c38a29c79cc7eae8f7efdb013e5a54c745ca205482b15b32b6"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"025120a7380541f0a2c553b791da8519":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_74d17b1f33f84f46aece8bdd4bcacf97","placeholder":"​","style":"IPY_MODEL_fdeac64f4d834bd7be61237f398a5827","tabbable":null,"tooltip":null,"value":" 36.8M/36.8M [00:00&lt;00:00, 49.9MB/s]"}},"0c5868b2eba845fa98aafea73e547af9":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3533615c8d2e4cc4b8e1ede462666575":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56b396dacb8340869b420d1168af9441":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b9887188f40463da8e5d41c1768b4c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_3533615c8d2e4cc4b8e1ede462666575","max":36757206,"min":0,"orientation":"horizontal","style":"IPY_MODEL_56b396dacb8340869b420d1168af9441","tabbable":null,"tooltip":null,"value":36757206}},"7375a885c8474eb8a30d42ed23c69c3f":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74d17b1f33f84f46aece8bdd4bcacf97":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9415c1a7a54541e3a3b470174a5588c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aa1ff725dcc8474a820d81d04d91d7b9","IPY_MODEL_5b9887188f40463da8e5d41c1768b4c0","IPY_MODEL_025120a7380541f0a2c553b791da8519"],"layout":"IPY_MODEL_0c5868b2eba845fa98aafea73e547af9","tabbable":null,"tooltip":null}},"aa1ff725dcc8474a820d81d04d91d7b9":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_7375a885c8474eb8a30d42ed23c69c3f","placeholder":"​","style":"IPY_MODEL_e43b089c9c7d4885b428e9dc2fd41d77","tabbable":null,"tooltip":null,"value":"model.safetensors: 100%"}},"e43b089c9c7d4885b428e9dc2fd41d77":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"fdeac64f4d834bd7be61237f398a5827":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport argparse\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split, KFold\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport warnings\nimport sys","metadata":{"execution":{"iopub.status.busy":"2025-12-06T13:53:51.889690Z","iopub.execute_input":"2025-12-06T13:53:51.889874Z","iopub.status.idle":"2025-12-06T13:54:01.149338Z","shell.execute_reply.started":"2025-12-06T13:53:51.889856Z","shell.execute_reply":"2025-12-06T13:54:01.148760Z"},"papermill":{"duration":66.686448,"end_time":"2025-11-28T11:17:29.600308","exception":false,"start_time":"2025-11-28T11:16:22.913860","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install torch torchvision pandas scikit-learn pillow tqdm timm\n!conda update torchvision","metadata":{"execution":{"iopub.status.busy":"2025-12-06T13:54:01.151017Z","iopub.execute_input":"2025-12-06T13:54:01.151323Z","iopub.status.idle":"2025-12-06T13:55:11.445927Z","shell.execute_reply.started":"2025-12-06T13:54:01.151305Z","shell.execute_reply":"2025-12-06T13:55:11.445208Z"},"papermill":{"duration":10.669048,"end_time":"2025-11-28T11:18:56.343467","exception":false,"start_time":"2025-11-28T11:18:45.674419","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.19)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.36.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (25.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (1.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.10.5)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n/bin/bash: line 1: conda: command not found\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nTOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\nUSERNAME = 'ada-yl2425'\nREPO_NAME = 'CSIRO---Image2Biomass-Prediction'\n!git clone https://{USERNAME}:{TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\n!git pull origin main\n!ls","metadata":{"papermill":{"duration":76.028035,"end_time":"2025-11-28T11:18:45.639404","exception":false,"start_time":"2025-11-28T11:17:29.611369","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:55:11.447088Z","iopub.execute_input":"2025-12-06T13:55:11.448304Z","iopub.status.idle":"2025-12-06T13:56:23.867247Z","shell.execute_reply.started":"2025-12-06T13:55:11.448272Z","shell.execute_reply":"2025-12-06T13:56:23.866293Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'CSIRO---Image2Biomass-Prediction'...\nremote: Enumerating objects: 710, done.\u001b[K\nremote: Counting objects: 100% (4/4), done.\u001b[K\nremote: Compressing objects: 100% (4/4), done.\u001b[K\nremote: Total 710 (delta 0), reused 0 (delta 0), pack-reused 706 (from 2)\u001b[K\nReceiving objects: 100% (710/710), 2.69 GiB | 47.25 MiB/s, done.\nResolving deltas: 100% (183/183), done.\nUpdating files: 100% (386/386), done.\nfatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\nCSIRO---Image2Biomass-Prediction\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"project_root = 'CSIRO---Image2Biomass-Prediction'\nif project_root not in sys.path:\n    sys.path.append(project_root)\n\nfrom KnowledgeDistillation.teacher_model import TeacherModel\nfrom KnowledgeDistillation.student_model import StudentModel\nfrom KnowledgeDistillation.loss import WeightedMSELoss, StudentLoss, calculate_weighted_r2\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\nfrom KnowledgeDistillation.data_transform import transForms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T13:56:23.868465Z","iopub.execute_input":"2025-12-06T13:56:23.868713Z","iopub.status.idle":"2025-12-06T13:56:27.547490Z","shell.execute_reply.started":"2025-12-06T13:56:23.868689Z","shell.execute_reply":"2025-12-06T13:56:27.546690Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# 忽略 PIL 的一些警告\nwarnings.filterwarnings(\"ignore\", \"(Possibly corrupt EXIF data|Truncated File Read)\")","metadata":{"execution":{"iopub.status.busy":"2025-12-06T13:56:27.548292Z","iopub.execute_input":"2025-12-06T13:56:27.548549Z","iopub.status.idle":"2025-12-06T13:56:27.552471Z","shell.execute_reply.started":"2025-12-06T13:56:27.548522Z","shell.execute_reply":"2025-12-06T13:56:27.551830Z"},"papermill":{"duration":0.043481,"end_time":"2025-11-28T11:18:56.423409","exception":false,"start_time":"2025-11-28T11:18:56.379928","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# --- 2. 自定义数据集 ---\n# (与 teacher_train 相同, Student 训练循环需要所有数据)\nclass PastureDataset(Dataset):\n\n    def __init__(self, df, img_dir, transforms, img_size): \n        self.df = df\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.img_size = img_size  \n\n        # 定义列名 (for teacher models) (improvement 3)\n        self.numeric_cols = ['Pre_GSHH_NDVI', 'Height_Ave_cm', 'month_sin', 'month_cos']\n        self.categorical_cols = ['State_encoded', 'Species_encoded']\n\n        self.log_target_cols = ['log_Dry_Green_g', 'log_Dry_Dead_g',\n                                'log_Dry_Clover_g', 'log_GDM_g', 'log_Dry_Total_g']\n        self.orig_target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g',\n                                 'GDM_g', 'Dry_Total_g']\n        \n        # improvement 3 deleted\n        '''self.log_teacher_cols = ['teacher_log_Dry_Green_g', 'teacher_log_Dry_Dead_g',\n                                 'teacher_log_Dry_Clover_g', 'teacher_log_GDM_g',\n                                 'teacher_log_Dry_Total_g']'''\n            \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        filename = row.name.split('/')[-1]\n        img_path = os.path.join(self.img_dir, filename)\n\n        try:\n            image = Image.open(img_path).convert('RGB')\n            image = self.transforms(image)\n        except Exception as e:\n            print(f\"Warning: Error loading image {img_path}. Using a dummy image. Error: {e}\")\n            image = torch.zeros((3, self.img_size, self.img_size))\n\n        # 2. 提取表格数据 (improvement 3)\n        numeric = torch.tensor(\n            row[self.numeric_cols].values.astype(np.float32),\n            dtype=torch.float32\n        )\n\n        categorical = torch.tensor(\n            row[self.categorical_cols].values.astype(np.int64), \n            dtype=torch.long\n        )\n\n        log_target = torch.tensor(\n            row[self.log_target_cols].values.astype(np.float32),\n            dtype=torch.float32\n        )\n        orig_target = torch.tensor(\n            row[self.orig_target_cols].values.astype(np.float32),\n            dtype=torch.float32\n        )\n\n        return {\n            'image': image, \n            'numeric': numeric, # improvement 3\n            'categorical': categorical, # improvement 3\n            'log_target': log_target,\n            'orig_target': orig_target\n        }","metadata":{"execution":{"iopub.status.busy":"2025-12-06T13:56:27.553326Z","iopub.execute_input":"2025-12-06T13:56:27.553598Z","iopub.status.idle":"2025-12-06T13:56:29.125351Z","shell.execute_reply.started":"2025-12-06T13:56:27.553574Z","shell.execute_reply":"2025-12-06T13:56:29.124633Z"},"papermill":{"duration":0.042746,"end_time":"2025-11-28T11:18:56.574132","exception":false,"start_time":"2025-11-28T11:18:56.531386","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# --- 3. 训练和验证循环 ---\n\ndef train_one_epoch_student(student_model, teacher_model, loader, \n                            criterion, optimizer, device):\n    student_model.train()\n    total_loss = 0.0\n\n    for batch in tqdm(loader, desc=\"Training\"):\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        image = batch['image']\n        numeric = batch['numeric']\n        categorical = batch['categorical']\n        log_target = batch['log_target']\n\n        # 梯度清零 (只为 Student)\n        optimizer.zero_grad()\n\n        # 1. 获取教师预测 (特征 + 软标签)\n        with torch.no_grad(): # 确保不计算教师的梯度\n            teacher_pred, teacher_features = teacher_model(image, numeric, categorical) # [B, 256]\n            # [B, 256] -> [B, 1, 256] -> [B, 5, 256]\n            teacher_features_expanded = teacher_features.unsqueeze(1).expand(-1, 5, -1)\n\n        # 2. 获取学生预测 (特征 + 预测)\n        student_pred, student_features = student_model(image) # Student 只需要图像\n        \n        # --- 修正 CosineEmbeddingLoss 的调用 ---\n        \n        # a. 获取批量大小 (B)\n        B = student_features.shape[0]\n        # b. 将特征从 3D 压平为 2D\n        # [B, 5, 256] -> [B*5, 256]\n        student_feat_flat = student_features.reshape(-1, student_features.shape[-1])\n        teacher_feat_flat = teacher_features_expanded.reshape(-1, teacher_features_expanded.shape[-1])\n        # c. 创建目标张量 (target tensor)\n        # 形状: [B*5]\n        target_tensor = torch.ones(B * 5, device=student_features.device)\n        \n        # 3. 计算蒸馏损失 (StudentLoss)\n        loss = criterion(student_pred, teacher_pred, student_feat_flat, \n                         teacher_feat_flat, target_tensor, log_target)\n\n        # 4. 反向传播 (只更新 Student 的权重)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(loader)\n\ndef validate_student(student_model, loader, criterion, device):\n\n    student_model.eval() # Student 进入评估模式\n    total_val_loss = 0.0\n    all_preds_orig = []\n    all_targets_orig = []\n\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Validating\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n\n            image = batch['image']\n            log_target = batch['log_target']\n            orig_target = batch['orig_target']\n\n            pred_log, _ = student_model(image)\n\n            loss = criterion(pred_log, log_target)\n            total_val_loss += loss.item()\n\n            # 转换回原始尺度\n            pred_orig = torch.expm1(pred_log)\n            all_preds_orig.append(pred_orig)\n            all_targets_orig.append(orig_target)\n\n    # 拼接所有批次的结果\n    all_preds_orig = torch.cat(all_preds_orig, dim=0)\n    all_targets_orig = torch.cat(all_targets_orig, dim=0)\n\n    # 计算 R2 (原始尺度)\n    val_r2 = calculate_weighted_r2(all_targets_orig, all_preds_orig, device)\n    avg_val_loss = total_val_loss / len(loader)\n\n    return avg_val_loss, val_r2","metadata":{"execution":{"iopub.status.busy":"2025-12-06T13:56:29.127170Z","iopub.execute_input":"2025-12-06T13:56:29.127485Z","iopub.status.idle":"2025-12-06T13:56:32.239320Z","shell.execute_reply.started":"2025-12-06T13:56:29.127463Z","shell.execute_reply":"2025-12-06T13:56:32.238521Z"},"papermill":{"duration":0.044711,"end_time":"2025-11-28T11:18:56.653469","exception":false,"start_time":"2025-11-28T11:18:56.608758","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# --- 4. 主函数 (Student K-Fold CV) ---\ndef main(args):\n    # 设置设备\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    # 加载数据\n    df = pd.read_csv(args.data_csv, index_col='image_path')\n\n    num_states = df['State_encoded'].nunique()\n    num_species = df['Species_encoded'].nunique()\n    print(f\"Found {num_states} states and {num_species} species.\")\n\n    # 图像预处理\n    train_transforms = transForms(args, num_bins=31, \n                        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],\n                        p=0.3, type='train')\n\n    val_transforms = transForms(args, num_bins=31, \n                        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],\n                        p=0.3, type='train')\n    # K-Fold CV 设置 （必须和teacher的保持一致！！！）\n    N_SPLITS = 5\n    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n    all_fold_best_r2 = []\n    \n    # 导入 LR 调度器\n    from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n\n    # K-Fold 训练循环\n    for fold, (train_indices, val_indices) in enumerate(kf.split(df)):\n        print(f\"========== FOLD {fold + 1}/{N_SPLITS} ==========\\n\")\n\n        # 1. 创建数据\n        train_df = df.iloc[train_indices]\n        val_df = df.iloc[val_indices]\n        train_dataset = PastureDataset(train_df, args.img_dir, train_transforms, args.img_size)\n        val_dataset = PastureDataset(val_df, args.img_dir, val_transforms, args.img_size)\n        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n\n        # 2. 重新初始化 Student Model 和优化器\n        student_model = StudentModel().to(device) \n\n        # --- MODIFICATION ---\n        # 2.5 加载对应的 Teacher Model (设为评估模式)\n        teacher_model = TeacherModel(num_states, num_species).to(device) \n        teacher_model_path = os.path.join(\n            args.teacher_model_dir,\n            f\"best_teacher_model_fold_{fold+1}.pth\"\n        )\n        teacher_model.load_state_dict(torch.load(teacher_model_path))\n        teacher_model.eval()\n        # --- END MODIFICATION ---\n\n\n        # 3. 始化损失函数\n        criterion_train = StudentLoss(alpha=args.alpha, beta=args.beta, gamma=args.gamma)\n        criterion_val = WeightedMSELoss()\n        \n        # --- MODIFICATION ---\n        # 新增一个用于特征匹配的损失\n        criterion_feature = nn.MSELoss()\n        # --- END MODIFICATION ---\n\n\n        # 4. 为 Student 设置差分学习率\n        head_param_names = [\n            'patch_projector',\n            'query_tokens',\n            'transformer_decoder',\n            'prediction_head'\n        ]\n        head_params = []\n        backbone_params = []\n\n        for name, param in student_model.named_parameters():\n            if not param.requires_grad:\n                continue\n            is_head = any(name.startswith(head_name) for head_name in head_param_names)\n            if is_head:\n                head_params.append(param)\n            else:\n                backbone_params.append(param)\n        \n        param_groups = [\n            {'params': backbone_params, 'lr': args.lr},      \n            {'params': head_params, 'lr': args.lr * 10} \n        ]\n        \n        optimizer = optim.AdamW(param_groups, lr=args.lr, weight_decay=1e-3)\n\n        # LR 调度器设置\n        TOTAL_EPOCHS = args.epochs \n        WARMUP_EPOCHS = 5 # 前 5 轮用于预热\n        \n        scheduler_warmup = LinearLR(optimizer, start_factor=0.1, total_iters=WARMUP_EPOCHS)\n        scheduler_cosine = CosineAnnealingLR(optimizer, T_max=(TOTAL_EPOCHS - WARMUP_EPOCHS), eta_min=1e-7)\n        scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_cosine], milestones=[WARMUP_EPOCHS])\n        \n        # 5. 训练循环\n        best_val_r2 = -float('inf')\n        patience_counter = 0\n\n        for epoch in range(args.epochs):\n            print(f\"--- Fold {fold+1}, Epoch {epoch+1}/{args.epochs} ---\")\n\n            train_loss = train_one_epoch_student(\n                            student_model,\n                            teacher_model,      # <-- 传递 teacher_model\n                            train_loader, \n                            criterion_train,    \n                            optimizer, \n                            device\n                        )\n\n            val_loss, val_r2 = validate_student(\n                student_model, val_loader, criterion_val, device\n            )\n\n            print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val R2: {val_r2:.4f}\")\n\n            scheduler.step()\n\n            if val_r2 > best_val_r2:\n                best_val_r2 = val_r2\n                patience_counter = 0\n                save_path = os.path.join(args.output_dir, f\"best_student_model_fold_{fold+1}.pth\")\n                torch.save(student_model.state_dict(), save_path)\n                print(f\"New best model for fold {fold+1} saved with R2: {best_val_r2:.4f}\")\n            else:\n                patience_counter += 1\n                print(f\"No improvement. Patience: {patience_counter}/{args.early_stopping_patience}\")\n\n            if patience_counter >= args.early_stopping_patience:\n                print(f\"--- Early stopping triggered at epoch {epoch+1} ---\")\n                break\n\n        print(f\"Fold {fold+1} complete. Best Validation R2: {best_val_r2:.4f}\")\n        all_fold_best_r2.append(best_val_r2)\n        print(\"=============================\\n\")\n\n\n    print(\"\\n--- Student K-Fold Cross-Validation Complete ---\")\n    print(f\"R2 scores for each fold: {all_fold_best_r2}\")\n    print(f\"Average R2: {np.mean(all_fold_best_r2):.4f}\")\n    print(f\"Std Dev R2: {np.std(all_fold_best_r2):.4f}\")","metadata":{"execution":{"iopub.status.busy":"2025-12-06T13:56:32.240171Z","iopub.execute_input":"2025-12-06T13:56:32.240494Z","iopub.status.idle":"2025-12-06T13:56:32.261251Z","shell.execute_reply.started":"2025-12-06T13:56:32.240441Z","shell.execute_reply":"2025-12-06T13:56:32.260666Z"},"papermill":{"duration":0.05522,"end_time":"2025-11-28T11:18:56.744951","exception":false,"start_time":"2025-11-28T11:18:56.689731","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Train Student Model via Distillation\") \n\n    # --- 路径 ---\n    parser.add_argument('--data_csv', type=str,\n                        default=os.path.join(project_root, 'outputs/datasets/train_processed.csv'))\n    parser.add_argument('--img_dir', type=str,\n                        default=os.path.join(project_root, 'csiro-biomass/train'))\n    parser.add_argument('--teacher_model_dir', type=str,\n                        default=os.path.join(project_root, 'outputs/models/teacher_model_output'))\n    \n    # Student 输出目录\n    output_path = os.path.join('kaggle/', 'working')\n    parser.add_argument('--output_dir', type=str,\n                        default=output_path,\n                        help='Directory to save the best student model')\n\n    # --- 训练超参数 ---\n    parser.add_argument('--img_size', type=int, default=260)\n    \n    # 学习率与 Teacher fine-tuning 时相同\n    parser.add_argument('--lr', type=float, default=5e-5, \n                        help='Base learning rate (Backbone)')\n    \n    parser.add_argument('--batch_size', type=int, default=16)\n    parser.add_argument('--epochs', type=int, default=150) \n    parser.add_argument('--num_workers', type=int, default=2)\n    parser.add_argument('--early_stopping_patience', type=int, default=15)\n\n    # --- 蒸馏超参数 ---\n    parser.add_argument('--alpha', type=float, default=0.5)\n    parser.add_argument('--beta', type=float, default=0.1)\n    parser.add_argument('--gamma', type=float, default=0.4)\n    \n    # ------------------------\n    args = parser.parse_args(args=[])\n    os.makedirs(args.output_dir, exist_ok=True)\n    print(f\"Student models will be saved to: {args.output_dir}\")\n    print(f\"Reading data from: {args.data_csv}\")\n\n    main(args)","metadata":{"execution":{"iopub.status.busy":"2025-12-06T13:56:32.261994Z","iopub.execute_input":"2025-12-06T13:56:32.262186Z","execution_failed":"2025-12-06T13:56:39.977Z"},"papermill":{"duration":3823.026684,"end_time":"2025-11-28T12:22:39.810326","exception":false,"start_time":"2025-11-28T11:18:56.783642","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Student models will be saved to: kaggle/working\nReading data from: CSIRO---Image2Biomass-Prediction/outputs/datasets/train_processed.csv\nUsing device: cuda\nFound 4 states and 15 species.\n========== FOLD 1/5 ==========\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/100M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e70c9fbec20a4304b51110eb72c0c7a8"}},"metadata":{}},{"name":"stdout","text":"--- Fold 1, Epoch 1/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/18 [00:00<?, ?it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}