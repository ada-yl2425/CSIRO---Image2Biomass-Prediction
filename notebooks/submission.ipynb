{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":668822,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":502875,"modelId":518028}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport pickle\nimport os\nimport sys\nimport warnings\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:48:20.288835Z","iopub.execute_input":"2025-12-01T17:48:20.289084Z","iopub.status.idle":"2025-12-01T17:48:33.937187Z","shell.execute_reply.started":"2025-12-01T17:48:20.289065Z","shell.execute_reply":"2025-12-01T17:48:33.936570Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"STUDENT_MODEL_PATH = '/kaggle/input/csiro-model/pytorch/default/2'\n\nRAW_DATA_DIR = \"/kaggle/input/csiro-biomass\"\nTEST_CSV = os.path.join(RAW_DATA_DIR, \"test.csv\")\nTEST_IMG_DIR = os.path.join(RAW_DATA_DIR, \"test\") \n\nTARGET_NAMES = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g', 'GDM_g', 'Dry_Total_g']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:48:52.730144Z","iopub.execute_input":"2025-12-01T17:48:52.730764Z","iopub.status.idle":"2025-12-01T17:48:52.735153Z","shell.execute_reply.started":"2025-12-01T17:48:52.730737Z","shell.execute_reply":"2025-12-01T17:48:52.734300Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, df_test, img_dir, transforms, img_size):\n        self.df = df_test\n        self.img_dir = img_dir \n        self.transforms = transforms\n        self.img_size = img_size\n        self.image_paths = df_test['image_path'].unique()\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path_index = self.image_paths[idx] \n        filename = img_path_index.split('/')[-1]\n        full_img_path = os.path.join(self.img_dir, filename)\n\n        try:\n            image = Image.open(full_img_path).convert('RGB')\n            image = self.transforms(image)\n        except Exception as e:\n            print(f\"Error loading {full_img_path}: {e}\")\n            image = torch.zeros((3, self.img_size, self.img_size))\n        \n        return {\n            'image_path': img_path_index, \n            'image': image\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:48:57.349028Z","iopub.execute_input":"2025-12-01T17:48:57.349621Z","iopub.status.idle":"2025-12-01T17:48:57.355465Z","shell.execute_reply.started":"2025-12-01T17:48:57.349595Z","shell.execute_reply":"2025-12-01T17:48:57.354685Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class StudentModel(nn.Module):\n\n    def __init__(self, img_model_name='efficientnet_b2'):\n        super(StudentModel, self).__init__()\n\n        # 1. Image Backbone\n        self.img_backbone = timm.create_model(\n            img_model_name,\n            pretrained=False, # different!!!\n            num_classes=0,\n            global_pool='' \n        )\n\n        self.num_img_features = self.img_backbone.num_features # 1408\n        self.embed_dim = 256 # embedding dims\n        self.num_heads = 8   # attention head\n        self.num_targets = 5 # 5 targets\n\n        # 1.2 Projector\n        self.patch_projector = nn.Linear(self.num_img_features, self.embed_dim)\n\n        # 2. Query\n        self.query_tokens = nn.Parameter(\n            torch.randn(1, self.num_targets, self.embed_dim)\n        )\n\n        # 2.2 Transformer\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=self.embed_dim,         # 256\n            nhead=self.num_heads,           # 8\n            dim_feedforward=self.embed_dim * 4, # 1024 (MLP  extension)\n            dropout=0.1,                    \n            batch_first=True\n        )\n        self.transformer_decoder = nn.TransformerDecoder(\n            decoder_layer, \n            num_layers=3  \n        )\n\n        # 3. Prediction Head\n        \n        self.prediction_head = nn.Sequential(\n            nn.LayerNorm(self.embed_dim),\n            nn.Linear(self.embed_dim, self.embed_dim * 2), # 256 -> 512\n            nn.ReLU(),\n            nn.Dropout(0.3), \n            nn.Linear(self.embed_dim * 2, 1) # 512 -> 1\n        )\n\n        # Selective Fine-tuning\n        for param in self.img_backbone.parameters():\n            param.requires_grad = False\n            \n        for param in self.img_backbone.blocks[-3:].parameters(): \n            param.requires_grad = True\n\n        if hasattr(self.img_backbone, 'conv_head'):\n             for param in self.img_backbone.conv_head.parameters():\n                  param.requires_grad = True\n        if hasattr(self.img_backbone, 'bn2'):\n             for param in self.img_backbone.bn2.parameters():\n                  param.requires_grad = True\n\n    def forward(self, image):\n        B = image.shape[0]\n\n        # 1. get feature map: [B, 1408, 8, 8]\n        x_map = self.img_backbone(image)\n\n        # 2. flat: [B, 64, 1408]\n        patches = x_map.flatten(2).permute(0, 2, 1) \n        \n        # 3. projector (get Key/Value)\n        # memory shape: [B, 64, 256]\n        memory = self.patch_projector(patches)\n\n        # 4. Query\n        # query shape: [B, 5, 256]\n        query = self.query_tokens.expand(B, -1, -1)\n\n        # 5. Transformer encoding\n        # tgt = query tokens [B, 5, 256]\n        # memory = [B, 64, 256]\n        # attn_output shape: [B, 5, 256]\n        attn_output = self.transformer_decoder(\n            tgt=query, \n            memory=memory\n        )\n\n        # 6. get prediction\n        # output shape: [B, 5, 1]\n        output = self.prediction_head(attn_output)\n        \n        # 7. squeeze: [B, 5, 1] -> [B, 5]\n        output = output.squeeze(-1)\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:53:10.941774Z","iopub.execute_input":"2025-12-01T17:53:10.942764Z","iopub.status.idle":"2025-12-01T17:53:10.957922Z","shell.execute_reply.started":"2025-12-01T17:53:10.942727Z","shell.execute_reply":"2025-12-01T17:53:10.956897Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ## 3. loading models\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nstudent_models = []\n\nfor i in range(1, 6):\n    model = StudentModel().to(device)\n    model.load_state_dict(\n        torch.load(\n            os.path.join(STUDENT_MODEL_PATH, f\"best_student_model_fold_{i}.pth\"),\n            map_location=device\n        )\n    )\n    model.eval()\n    student_models.append(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:56:22.657539Z","iopub.execute_input":"2025-12-01T17:56:22.658057Z","iopub.status.idle":"2025-12-01T17:56:27.063068Z","shell.execute_reply.started":"2025-12-01T17:56:22.658020Z","shell.execute_reply":"2025-12-01T17:56:27.062294Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ## 4. handling test dataset\n\ndf_test = pd.read_csv(TEST_CSV)\ndf_test = df_test[['image_path']].drop_duplicates()\nprint(f\"Found {len(df_test)} unique test images.\")\n\n\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((260, 260)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntest_dataset = TestDataset(df_test, TEST_IMG_DIR, test_transforms, 260)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n\nprint(\"Test DataLoader created.\")\n\n\n\nprint(\"Running five student models on test set...\")\nfor model in student_models:\n    model.eval()\n\nall_preds = []\nall_image_paths = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Running Ensemble Inference\"):\n        image = batch['image'].to(device)\n\n        batch_preds = []\n        for i, model in enumerate(student_models):\n            pred_log = model(image)\n            pred_orig = torch.expm1(pred_log)\n            batch_preds.append(pred_orig)\n        \n        batch_preds = torch.stack(batch_preds, dim=0)  \n        ensemble_pred = torch.mean(batch_preds, dim=0)  \n        \n        all_preds.append(ensemble_pred.cpu())\n        all_image_paths.extend(batch['image_path'])\n\npreds_tensor = torch.cat(all_preds, dim=0).numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:59:04.749700Z","iopub.execute_input":"2025-12-01T17:59:04.750392Z","iopub.status.idle":"2025-12-01T17:59:06.060443Z","shell.execute_reply.started":"2025-12-01T17:59:04.750365Z","shell.execute_reply":"2025-12-01T17:59:06.059708Z"}},"outputs":[{"name":"stdout","text":"Found 1 unique test images.\nTest DataLoader created.\nRunning five student models on test set...\n","output_type":"stream"},{"name":"stderr","text":"Running Ensemble Inference: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]","output_type":"stream"},{"name":"stdout","text":"Inference complete. 预测形状: (1, 5)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Formatting submission file\n\ndf_preds_wide = pd.DataFrame(preds_tensor, columns=TARGET_NAMES)\ndf_preds_wide['image_path'] = all_image_paths\n\n\ndf_preds_long = df_preds_wide.melt(\n    id_vars='image_path', \n    var_name='target_name', \n    value_name='target'\n)\n\n\ndf_preds_long['image_id'] = df_preds_long['image_path'].apply(lambda x: x.split('/')[-1].split('.')[0])\ndf_preds_long['sample_id'] = df_preds_long['image_id'] + '__' + df_preds_long['target_name']\n\n\nsubmission_df = df_preds_long[['sample_id', 'target']]\n\n\nsubmission_df.loc[:, 'target'] = submission_df['target'].clip(lower=0)\n\n\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"submission.csv created successfully!\")\nprint(\"Submission file head:\")\nprint(submission_df.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T17:59:16.280773Z","iopub.execute_input":"2025-12-01T17:59:16.281460Z","iopub.status.idle":"2025-12-01T17:59:16.307970Z","shell.execute_reply.started":"2025-12-01T17:59:16.281428Z","shell.execute_reply":"2025-12-01T17:59:16.307199Z"}},"outputs":[{"name":"stdout","text":"Formatting submission file...\nsubmission.csv created successfully!\nSubmission file head:\n                    sample_id     target\n0   ID1001187975__Dry_Green_g  23.954681\n1    ID1001187975__Dry_Dead_g  21.282005\n2  ID1001187975__Dry_Clover_g   0.745209\n3         ID1001187975__GDM_g  24.372406\n4   ID1001187975__Dry_Total_g  49.692810\n","output_type":"stream"}],"execution_count":10}]}