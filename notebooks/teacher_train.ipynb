{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport argparse\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split, KFold\nfrom torchvision import transforms\nfrom PIL import Image\nfrom tqdm import tqdm\nimport warnings\nimport sys","metadata":{"id":"9uB5tYNroaTz","outputId":"f7a68b56-6346-40cf-8ddf-74f701504297","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T11:37:23.135131Z","iopub.execute_input":"2025-12-06T11:37:23.135737Z","iopub.status.idle":"2025-12-06T11:37:23.140289Z","shell.execute_reply.started":"2025-12-06T11:37:23.135712Z","shell.execute_reply":"2025-12-06T11:37:23.139593Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!pip install torch torchvision pandas scikit-learn pillow tqdm timm\n!conda update torchvision","metadata":{"id":"yqKgiPaVtEkT","outputId":"60e3e59c-8240-47a7-a3f0-f830a9bd08de","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T11:37:33.797552Z","iopub.execute_input":"2025-12-06T11:37:33.798105Z","iopub.status.idle":"2025-12-06T11:37:33.934023Z","shell.execute_reply.started":"2025-12-06T11:37:33.798081Z","shell.execute_reply":"2025-12-06T11:37:33.933255Z"}},"outputs":[{"name":"stdout","text":"/bin/bash: line 1: conda: command not found\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nTOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\nUSERNAME = 'ada-yl2425'\nREPO_NAME = 'CSIRO---Image2Biomass-Prediction'\n!git clone https://{USERNAME}:{TOKEN}@github.com/{USERNAME}/{REPO_NAME}.git\n!git pull origin main\n!ls","metadata":{"id":"RtVfvpPytA2A","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T11:35:59.998107Z","iopub.execute_input":"2025-12-06T11:35:59.998395Z","iopub.status.idle":"2025-12-06T11:36:07.382503Z","shell.execute_reply.started":"2025-12-06T11:35:59.998368Z","shell.execute_reply":"2025-12-06T11:36:07.381869Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"project_root = 'CSIRO---Image2Biomass-Prediction'\nif project_root not in sys.path:\n    sys.path.append(project_root)\n\nfrom KnowledgeDistillation.teacher_model import TeacherModel\nfrom KnowledgeDistillation.loss import WeightedMSELoss, calculate_weighted_r2\nfrom KnowledgeDistillation.data_transform import transForms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T11:38:09.500254Z","iopub.execute_input":"2025-12-06T11:38:09.500557Z","iopub.status.idle":"2025-12-06T11:38:09.505015Z","shell.execute_reply.started":"2025-12-06T11:38:09.500534Z","shell.execute_reply":"2025-12-06T11:38:09.504232Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# 忽略 PIL 的一些警告\nwarnings.filterwarnings(\"ignore\", \"(Possibly corrupt EXIF data|Truncated File Read)\")","metadata":{"id":"rKsGzyteuxmo","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T11:36:07.383990Z","iopub.execute_input":"2025-12-06T11:36:07.384326Z","iopub.status.idle":"2025-12-06T11:36:07.388293Z","shell.execute_reply.started":"2025-12-06T11:36:07.384308Z","shell.execute_reply":"2025-12-06T11:36:07.387400Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class PastureDataset(Dataset):\n\n    def __init__(self, df, img_dir, transforms, img_size): \n        self.df = df\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.img_size = img_size  \n\n        # 定义列名\n        self.numeric_cols = ['Pre_GSHH_NDVI', 'Height_Ave_cm', 'month_sin', 'month_cos']\n        self.categorical_cols = ['State_encoded', 'Species_encoded']\n\n        # 训练目标 (log scale)\n        self.log_target_cols = ['log_Dry_Green_g', 'log_Dry_Dead_g',\n                                'log_Dry_Clover_g', 'log_GDM_g', 'log_Dry_Total_g']\n\n        # 验证目标 (original scale)\n        self.orig_target_cols = ['Dry_Green_g', 'Dry_Dead_g', 'Dry_Clover_g',\n                                 'GDM_g', 'Dry_Total_g']\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        # 1. 加载图像\n        filename = row.name.split('/')[-1]\n        img_path = os.path.join(self.img_dir, filename)\n\n        try:\n            image = Image.open(img_path).convert('RGB')\n            image = self.transforms(image)\n        except Exception as e:\n            print(f\"Warning: Error loading image {img_path}. Using a dummy image. Error: {e}\")\n            image = torch.zeros((3, self.img_size, self.img_size))\n\n        # 2. 提取表格数据\n        numeric = torch.tensor(\n            row[self.numeric_cols].values.astype(np.float32),\n            dtype=torch.float32\n        )\n\n        categorical = torch.tensor(\n            row[self.categorical_cols].values.astype(np.int64), \n            dtype=torch.long\n        )\n\n        # 3. 提取目标\n        log_target = torch.tensor(\n            row[self.log_target_cols].values.astype(np.float32),\n            dtype=torch.float32\n        )\n\n        orig_target = torch.tensor(\n            row[self.orig_target_cols].values.astype(np.float32),\n            dtype=torch.float32\n        )\n\n        return {\n            'image': image,\n            'numeric': numeric,\n            'categorical': categorical,\n            'log_target': log_target,\n            'orig_target': orig_target\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T11:36:07.389333Z","iopub.execute_input":"2025-12-06T11:36:07.389665Z","iopub.status.idle":"2025-12-06T11:36:07.412220Z","shell.execute_reply.started":"2025-12-06T11:36:07.389633Z","shell.execute_reply":"2025-12-06T11:36:07.411458Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# --- 3. 训练和验证循环 ---\n\ndef train_one_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0.0\n\n    for batch in tqdm(loader, desc=\"Training\"):\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        image = batch['image']\n        numeric = batch['numeric']\n        categorical = batch['categorical']\n        log_target = batch['log_target']\n        \n        # 梯度清零\n        optimizer.zero_grad()\n\n        # 前向传播\n        pred, features = model(image, numeric, categorical)\n\n        # 计算损失 (在 log 尺度上)\n        loss = criterion(pred, log_target)\n\n        # 反向传播\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(loader)\n\ndef validate(model, loader, criterion, device):\n    model.eval()\n    total_val_loss = 0.0\n    all_preds_orig = []\n    all_targets_orig = []\n\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Validating\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n\n            image = batch['image']\n            numeric = batch['numeric']\n            categorical = batch['categorical']\n            log_target = batch['log_target']\n            orig_target = batch['orig_target']\n\n            # 预测 (log 尺度)\n            pred_log, _ = model(image, numeric, categorical)\n\n            # 计算验证损失 (log 尺度)\n            loss = criterion(pred_log, log_target)\n            total_val_loss += loss.item()\n\n            # 转换回原始尺度\n            pred_orig = torch.expm1(pred_log)\n\n            all_preds_orig.append(pred_orig)\n            all_targets_orig.append(orig_target)\n\n    # 拼接所有批次的结果\n    all_preds_orig = torch.cat(all_preds_orig, dim=0)\n    all_targets_orig = torch.cat(all_targets_orig, dim=0)\n\n    # 计算 R2 (原始尺度)\n    val_r2 = calculate_weighted_r2(all_targets_orig, all_preds_orig, device)\n\n    avg_val_loss = total_val_loss / len(loader)\n\n    return avg_val_loss, val_r2\n","metadata":{"id":"ZqcjtP-5vTNp","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T11:36:07.412974Z","iopub.execute_input":"2025-12-06T11:36:07.413203Z","iopub.status.idle":"2025-12-06T11:36:07.430126Z","shell.execute_reply.started":"2025-12-06T11:36:07.413184Z","shell.execute_reply":"2025-12-06T11:36:07.429532Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# --- 4. 主函数 (已更新为 5-Fold CV + Early Stopping) ---\ndef main(args):\n    device = 'cpu'\n    if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n        print(\"Cuda installed! Running on GPU!\")\n        device = 'cuda'\n    else:\n        print(\"No GPU available!\")\n\n    # 加载数据\n    df = pd.read_csv(args.data_csv, index_col='image_path')\n\n    # 获取类别数量 (用于 Embedding)\n    num_states = df['State_encoded'].nunique()\n    num_species = df['Species_encoded'].nunique()\n    print(f\"Found {num_states} states and {num_species} species.\")\n\n    # 图像预处理\n    train_transforms = transForms(args, num_bins=31, \n                        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],\n                        p=0.3, type='train')\n\n    val_transforms = transForms(args, num_bins=31, \n                        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],\n                        p=0.3, type='train')\n\n    # --- K-Fold Cross-Validation 设置 ---\n    N_SPLITS = 5\n    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n    all_fold_best_r2 = [] # 存储每一折的 R2 分数\n\n    # --- K-Fold 训练循环 ---\n    for fold, (train_indices, val_indices) in enumerate(kf.split(df)):\n        print(f\"========== FOLD {fold + 1}/{N_SPLITS} ==========\")\n\n        # 1. 为当前折创建数据\n        train_df = df.iloc[train_indices]\n        val_df = df.iloc[val_indices]\n\n        # 2. 创建 Datasets 和 DataLoaders\n        train_dataset = PastureDataset(train_df, args.img_dir, train_transforms, args.img_size)\n        val_dataset = PastureDataset(val_df, args.img_dir, val_transforms, args.img_size)\n\n        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)\n        val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n\n        # 3. 为当前折重新初始化模型、损失和优化器\n        model = TeacherModel(num_states, num_species).to(device)\n        criterion = WeightedMSELoss().to(device)\n        \n        # 设置差分学习率 (Differential LRs)\n        \n        # 1. 定义哪些模块属于“Head”（从零开始学）\n        head_param_names = [\n            'tabular_embedder',          # 包含 state_embedding 和 species_embedding\n            'tabular_processor',         # tab_mlp\n            'tab_self_attn',             # 表格自注意力层\n            'img_kv_projector',          # 图像特征投影\n            'tab_q_projector',           # 查询投影\n            'cross_attention_blocks',    # 交叉注意力块\n            'tab_attn_norm',             # 表格注意力归一化\n            'fusion_head'                # 融合头\n        ]        \n \n        head_params = []\n        backbone_params = []\n\n        # 2. 将所有可训练参数 (requires_grad=True) 分配到两组\n        for name, param in model.named_parameters():\n            if not param.requires_grad:\n                continue\n                \n            is_head = False\n            for head_name in head_param_names:\n                if name.startswith(head_name):\n                    head_params.append(param)\n                    is_head = True\n                    break\n            \n            if not is_head:\n                backbone_params.append(param)\n\n        # 3. 创建参数组\n        # 主干 (Backbone) 使用基础 LR (例如 5e-5)\n        # 头部 (Head) 使用 10 倍的基础 LR (例如 5e-4)\n        param_groups = [\n            {'params': backbone_params, 'lr': args.lr}, \n            {'params': head_params, 'lr': args.lr * 10}  \n        ]\n\n        # 4. 为当前折重新初始化模型、损失和优化器\n        \n        # 将 param_groups 传入优化器\n        optimizer = optim.AdamW(param_groups, \n                              lr=args.lr, # (默认 LR，主要由 group 覆盖)\n                              weight_decay=1e-3) \n\n        # -----------------------------------------------------        \n\n        # 学习率调度器\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=7, factor=0.1)\n\n        # 5. 训练循环 (针对当前折)\n        best_val_r2 = -float('inf')\n\n        # 早停变量\n        patience_counter = 0\n        # -------------------------\n\n        for epoch in range(args.epochs):\n            print(f\"--- Fold {fold+1}, Epoch {epoch+1}/{args.epochs} ---\")\n\n            train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n            val_loss, val_r2 = validate(model, val_loader, criterion, device)\n\n            print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val R2: {val_r2:.4f}\")\n\n            # 更新学习率\n            scheduler.step(val_r2)\n\n            # 早停和模型保存逻辑\n            if val_r2 > best_val_r2:\n                best_val_r2 = val_r2\n                patience_counter = 0 # 重置耐心\n\n                # 保存最佳模型 (针对当前折)\n                save_path = os.path.join(args.output_dir, f\"best_teacher_model_fold_{fold+1}.pth\")\n                torch.save(model.state_dict(), save_path)\n                print(f\"New best model for fold {fold+1} saved with R2: {best_val_r2:.4f}\")\n            else:\n                patience_counter += 1 # 增加耐心\n                print(f\"No improvement. Patience: {patience_counter}/{args.early_stopping_patience}\")\n\n            # 检查是否触发早停\n            if patience_counter >= args.early_stopping_patience:\n                print(f\"--- Early stopping triggered at epoch {epoch+1} ---\")\n                break # 跳出当前 fold 的 epoch 循环\n            # -----------------------------------\n\n        print(f\"Fold {fold+1} complete. Best Validation R2: {best_val_r2:.4f}\")\n        all_fold_best_r2.append(best_val_r2)\n        print(\"=============================\\n\")\n\n    # --- K-Fold 结束后，计算并打印平均 R2 ---\n    print(\"\\n--- K-Fold Cross-Validation Complete ---\")\n    print(f\"R2 scores for each fold: {all_fold_best_r2}\")\n    print(f\"Average R2: {np.mean(all_fold_best_r2):.4f}\")\n    print(f\"Std Dev R2: {np.std(all_fold_best_r2):.4f}\")","metadata":{"id":"YOFG9A4k_brg","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T11:43:47.245363Z","iopub.execute_input":"2025-12-06T11:43:47.245679Z","iopub.status.idle":"2025-12-06T11:43:47.259377Z","shell.execute_reply.started":"2025-12-06T11:43:47.245657Z","shell.execute_reply":"2025-12-06T11:43:47.258821Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Train Teacher Model\")\n\n    parser.add_argument('--data_csv', type=str,\n                        default=os.path.join(project_root, 'outputs/datasets/train_processed.csv'))\n    parser.add_argument('--img_dir', type=str,\n                        default=os.path.join(project_root, 'csiro-biomass/train'))\n\n    # 指定一个明确的输出目录\n    output_path = os.path.join('/kaggle/', 'working')\n    parser.add_argument('--output_dir', type=str,\n                        default=output_path,\n                        help='Directory to save the best model')\n\n    # 训练超参数\n    parser.add_argument('--img_size', type=int, default=260, \n                        help='Image size for the model (B2 uses 260)')\n    parser.add_argument('--lr', type=float, default=5e-5,\n                        help='Initial learning rate (1e-4 is good for fine-tuning)')\n    parser.add_argument('--batch_size', type=int, default=16,\n                        help='Batch size (use 8 or 16 for small datasets)')\n    parser.add_argument('--epochs', type=int, default=150,\n                        help='Number of training epochs')\n    parser.add_argument('--val_split', type=float, default=0.2,\n                        help='Validation split fraction')\n    parser.add_argument('--num_workers', type=int, default=2,\n                        help='Number of workers for DataLoader')\n\n    # --- 早停参数 ---\n    parser.add_argument('--early_stopping_patience', type=int, default=15,\n                        help='Patience for early stopping (e.g., 15 epochs)')\n    # -------------------------\n\n    args = parser.parse_args(args=[])\n\n    # 确保输出目录存在\n    os.makedirs(args.output_dir, exist_ok=True)\n    print(f\"Model output will be saved to: {args.output_dir}\")\n    print(f\"Reading data from: {args.data_csv}\")\n\n    main(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T12:12:41.070407Z","iopub.execute_input":"2025-12-06T12:12:41.071029Z","iopub.status.idle":"2025-12-06T12:52:44.482540Z","shell.execute_reply.started":"2025-12-06T12:12:41.070996Z","shell.execute_reply":"2025-12-06T12:52:44.481589Z"}},"outputs":[{"name":"stdout","text":"Model output will be saved to: /kaggle/working\nReading data from: CSIRO---Image2Biomass-Prediction/outputs/datasets/train_processed.csv\nCuda installed! Running on GPU!\nFound 4 states and 15 species.\n========== FOLD 1/5 ==========\n--- Fold 1, Epoch 1/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss: 1.0250 | Val Loss: 0.1906 | Val R2: 0.1198\nNew best model for fold 1 saved with R2: 0.1198\n--- Fold 1, Epoch 2/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss: 0.1511 | Val Loss: 0.0888 | Val R2: 0.3134\nNew best model for fold 1 saved with R2: 0.3134\n--- Fold 1, Epoch 3/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss: 0.1061 | Val Loss: 0.0811 | Val R2: 0.2335\nNo improvement. Patience: 1/15\n--- Fold 1, Epoch 4/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.11it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss: 0.0933 | Val Loss: 0.1253 | Val R2: -0.3278\nNo improvement. Patience: 2/15\n--- Fold 1, Epoch 5/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss: 0.1011 | Val Loss: 0.0557 | Val R2: 0.6488\nNew best model for fold 1 saved with R2: 0.6488\n--- Fold 1, Epoch 6/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss: 0.0749 | Val Loss: 0.0467 | Val R2: 0.6835\nNew best model for fold 1 saved with R2: 0.6835\n--- Fold 1, Epoch 7/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss: 0.0668 | Val Loss: 0.0509 | Val R2: 0.7050\nNew best model for fold 1 saved with R2: 0.7050\n--- Fold 1, Epoch 8/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss: 0.0670 | Val Loss: 0.0461 | Val R2: 0.7098\nNew best model for fold 1 saved with R2: 0.7098\n--- Fold 1, Epoch 9/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss: 0.0735 | Val Loss: 0.0619 | Val R2: 0.6233\nNo improvement. Patience: 1/15\n--- Fold 1, Epoch 10/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss: 0.0716 | Val Loss: 0.0571 | Val R2: 0.6076\nNo improvement. Patience: 2/15\n--- Fold 1, Epoch 11/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.11it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss: 0.0852 | Val Loss: 0.0470 | Val R2: 0.6767\nNo improvement. Patience: 3/15\n--- Fold 1, Epoch 12/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss: 0.0787 | Val Loss: 0.0512 | Val R2: 0.6521\nNo improvement. Patience: 4/15\n--- Fold 1, Epoch 13/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss: 0.0609 | Val Loss: 0.0565 | Val R2: 0.6744\nNo improvement. Patience: 5/15\n--- Fold 1, Epoch 14/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss: 0.0570 | Val Loss: 0.0543 | Val R2: 0.6922\nNo improvement. Patience: 6/15\n--- Fold 1, Epoch 15/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.14it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss: 0.0536 | Val Loss: 0.0504 | Val R2: 0.6591\nNo improvement. Patience: 7/15\n--- Fold 1, Epoch 16/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss: 0.0567 | Val Loss: 0.0531 | Val R2: 0.6048\nNo improvement. Patience: 8/15\n--- Fold 1, Epoch 17/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss: 0.0464 | Val Loss: 0.0446 | Val R2: 0.7641\nNew best model for fold 1 saved with R2: 0.7641\n--- Fold 1, Epoch 18/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss: 0.0483 | Val Loss: 0.0434 | Val R2: 0.7855\nNew best model for fold 1 saved with R2: 0.7855\n--- Fold 1, Epoch 19/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.14it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss: 0.0468 | Val Loss: 0.0484 | Val R2: 0.7058\nNo improvement. Patience: 1/15\n--- Fold 1, Epoch 20/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss: 0.0470 | Val Loss: 0.0469 | Val R2: 0.7544\nNo improvement. Patience: 2/15\n--- Fold 1, Epoch 21/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss: 0.0460 | Val Loss: 0.0436 | Val R2: 0.7614\nNo improvement. Patience: 3/15\n--- Fold 1, Epoch 22/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss: 0.0453 | Val Loss: 0.0457 | Val R2: 0.7332\nNo improvement. Patience: 4/15\n--- Fold 1, Epoch 23/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.08it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss: 0.0452 | Val Loss: 0.0459 | Val R2: 0.7622\nNo improvement. Patience: 5/15\n--- Fold 1, Epoch 24/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss: 0.0455 | Val Loss: 0.0477 | Val R2: 0.7385\nNo improvement. Patience: 6/15\n--- Fold 1, Epoch 25/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.17it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss: 0.0434 | Val Loss: 0.0437 | Val R2: 0.7682\nNo improvement. Patience: 7/15\n--- Fold 1, Epoch 26/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss: 0.0422 | Val Loss: 0.0423 | Val R2: 0.7860\nNew best model for fold 1 saved with R2: 0.7860\n--- Fold 1, Epoch 27/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.17it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss: 0.0409 | Val Loss: 0.0418 | Val R2: 0.7694\nNo improvement. Patience: 1/15\n--- Fold 1, Epoch 28/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss: 0.0452 | Val Loss: 0.0434 | Val R2: 0.7515\nNo improvement. Patience: 2/15\n--- Fold 1, Epoch 29/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.11it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss: 0.0451 | Val Loss: 0.0423 | Val R2: 0.7589\nNo improvement. Patience: 3/15\n--- Fold 1, Epoch 30/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss: 0.0462 | Val Loss: 0.0454 | Val R2: 0.7300\nNo improvement. Patience: 4/15\n--- Fold 1, Epoch 31/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss: 0.0448 | Val Loss: 0.0548 | Val R2: 0.7042\nNo improvement. Patience: 5/15\n--- Fold 1, Epoch 32/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss: 0.0470 | Val Loss: 0.0505 | Val R2: 0.7395\nNo improvement. Patience: 6/15\n--- Fold 1, Epoch 33/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss: 0.0408 | Val Loss: 0.0451 | Val R2: 0.7660\nNo improvement. Patience: 7/15\n--- Fold 1, Epoch 34/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss: 0.0428 | Val Loss: 0.0490 | Val R2: 0.7408\nNo improvement. Patience: 8/15\n--- Fold 1, Epoch 35/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss: 0.0403 | Val Loss: 0.0475 | Val R2: 0.7514\nNo improvement. Patience: 9/15\n--- Fold 1, Epoch 36/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss: 0.0429 | Val Loss: 0.0455 | Val R2: 0.7650\nNo improvement. Patience: 10/15\n--- Fold 1, Epoch 37/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss: 0.0463 | Val Loss: 0.0518 | Val R2: 0.7324\nNo improvement. Patience: 11/15\n--- Fold 1, Epoch 38/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.04it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss: 0.0473 | Val Loss: 0.0463 | Val R2: 0.7303\nNo improvement. Patience: 12/15\n--- Fold 1, Epoch 39/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss: 0.0421 | Val Loss: 0.0446 | Val R2: 0.7584\nNo improvement. Patience: 13/15\n--- Fold 1, Epoch 40/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss: 0.0427 | Val Loss: 0.0447 | Val R2: 0.7342\nNo improvement. Patience: 14/15\n--- Fold 1, Epoch 41/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss: 0.0390 | Val Loss: 0.0435 | Val R2: 0.7447\nNo improvement. Patience: 15/15\n--- Early stopping triggered at epoch 41 ---\nFold 1 complete. Best Validation R2: 0.7860\n=============================\n\n========== FOLD 2/5 ==========\n--- Fold 2, Epoch 1/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  1.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss: 1.1038 | Val Loss: 0.2854 | Val R2: -0.1936\nNew best model for fold 2 saved with R2: -0.1936\n--- Fold 2, Epoch 2/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss: 0.1669 | Val Loss: 0.1175 | Val R2: 0.0897\nNew best model for fold 2 saved with R2: 0.0897\n--- Fold 2, Epoch 3/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss: 0.1072 | Val Loss: 0.0654 | Val R2: 0.5851\nNew best model for fold 2 saved with R2: 0.5851\n--- Fold 2, Epoch 4/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss: 0.0911 | Val Loss: 0.0524 | Val R2: 0.6778\nNew best model for fold 2 saved with R2: 0.6778\n--- Fold 2, Epoch 5/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:09<00:00,  1.99it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss: 0.0806 | Val Loss: 0.0525 | Val R2: 0.7062\nNew best model for fold 2 saved with R2: 0.7062\n--- Fold 2, Epoch 6/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.04it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  1.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss: 0.0853 | Val Loss: 0.0556 | Val R2: 0.6624\nNo improvement. Patience: 1/15\n--- Fold 2, Epoch 7/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  1.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss: 0.0900 | Val Loss: 0.0434 | Val R2: 0.7014\nNo improvement. Patience: 2/15\n--- Fold 2, Epoch 8/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.04it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  1.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss: 0.0792 | Val Loss: 0.0512 | Val R2: 0.6896\nNo improvement. Patience: 3/15\n--- Fold 2, Epoch 9/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss: 0.0630 | Val Loss: 0.0549 | Val R2: 0.5978\nNo improvement. Patience: 4/15\n--- Fold 2, Epoch 10/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.02it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss: 0.0718 | Val Loss: 0.0806 | Val R2: 0.3796\nNo improvement. Patience: 5/15\n--- Fold 2, Epoch 11/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss: 0.0662 | Val Loss: 0.0605 | Val R2: 0.5082\nNo improvement. Patience: 6/15\n--- Fold 2, Epoch 12/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.02it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss: 0.0733 | Val Loss: 0.0524 | Val R2: 0.5801\nNo improvement. Patience: 7/15\n--- Fold 2, Epoch 13/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss: 0.0642 | Val Loss: 0.0637 | Val R2: 0.4298\nNo improvement. Patience: 8/15\n--- Fold 2, Epoch 14/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss: 0.0519 | Val Loss: 0.0532 | Val R2: 0.5491\nNo improvement. Patience: 9/15\n--- Fold 2, Epoch 15/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss: 0.0510 | Val Loss: 0.0471 | Val R2: 0.6401\nNo improvement. Patience: 10/15\n--- Fold 2, Epoch 16/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.04it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  1.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss: 0.0472 | Val Loss: 0.0421 | Val R2: 0.6597\nNo improvement. Patience: 11/15\n--- Fold 2, Epoch 17/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:09<00:00,  1.99it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss: 0.0539 | Val Loss: 0.0466 | Val R2: 0.6357\nNo improvement. Patience: 12/15\n--- Fold 2, Epoch 18/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.02it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss: 0.0523 | Val Loss: 0.0441 | Val R2: 0.6015\nNo improvement. Patience: 13/15\n--- Fold 2, Epoch 19/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.02it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss: 0.0512 | Val Loss: 0.0417 | Val R2: 0.6916\nNo improvement. Patience: 14/15\n--- Fold 2, Epoch 20/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.08it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss: 0.0507 | Val Loss: 0.0424 | Val R2: 0.7027\nNo improvement. Patience: 15/15\n--- Early stopping triggered at epoch 20 ---\nFold 2 complete. Best Validation R2: 0.7062\n=============================\n\n========== FOLD 3/5 ==========\n--- Fold 3, Epoch 1/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.02it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss: 1.1175 | Val Loss: 0.1670 | Val R2: 0.2538\nNew best model for fold 3 saved with R2: 0.2538\n--- Fold 3, Epoch 2/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss: 0.2336 | Val Loss: 0.0750 | Val R2: 0.5430\nNew best model for fold 3 saved with R2: 0.5430\n--- Fold 3, Epoch 3/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss: 0.1499 | Val Loss: 0.0853 | Val R2: 0.4603\nNo improvement. Patience: 1/15\n--- Fold 3, Epoch 4/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss: 0.1089 | Val Loss: 0.0708 | Val R2: 0.2917\nNo improvement. Patience: 2/15\n--- Fold 3, Epoch 5/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss: 0.0868 | Val Loss: 0.0450 | Val R2: 0.7555\nNew best model for fold 3 saved with R2: 0.7555\n--- Fold 3, Epoch 6/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss: 0.0914 | Val Loss: 0.0526 | Val R2: 0.4825\nNo improvement. Patience: 1/15\n--- Fold 3, Epoch 7/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss: 0.0731 | Val Loss: 0.0509 | Val R2: 0.7116\nNo improvement. Patience: 2/15\n--- Fold 3, Epoch 8/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.02it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss: 0.0640 | Val Loss: 0.0483 | Val R2: 0.7478\nNo improvement. Patience: 3/15\n--- Fold 3, Epoch 9/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss: 0.0617 | Val Loss: 0.0453 | Val R2: 0.7488\nNo improvement. Patience: 4/15\n--- Fold 3, Epoch 10/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss: 0.0651 | Val Loss: 0.0500 | Val R2: 0.7134\nNo improvement. Patience: 5/15\n--- Fold 3, Epoch 11/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss: 0.0626 | Val Loss: 0.0506 | Val R2: 0.7199\nNo improvement. Patience: 6/15\n--- Fold 3, Epoch 12/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss: 0.0602 | Val Loss: 0.0419 | Val R2: 0.7535\nNo improvement. Patience: 7/15\n--- Fold 3, Epoch 13/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss: 0.0631 | Val Loss: 0.0526 | Val R2: 0.6808\nNo improvement. Patience: 8/15\n--- Fold 3, Epoch 14/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss: 0.0571 | Val Loss: 0.0423 | Val R2: 0.8002\nNew best model for fold 3 saved with R2: 0.8002\n--- Fold 3, Epoch 15/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss: 0.0486 | Val Loss: 0.0400 | Val R2: 0.8059\nNew best model for fold 3 saved with R2: 0.8059\n--- Fold 3, Epoch 16/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.02it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss: 0.0500 | Val Loss: 0.0414 | Val R2: 0.7881\nNo improvement. Patience: 1/15\n--- Fold 3, Epoch 17/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss: 0.0472 | Val Loss: 0.0406 | Val R2: 0.7901\nNo improvement. Patience: 2/15\n--- Fold 3, Epoch 18/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.08it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss: 0.0516 | Val Loss: 0.0418 | Val R2: 0.7507\nNo improvement. Patience: 3/15\n--- Fold 3, Epoch 19/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss: 0.0508 | Val Loss: 0.0426 | Val R2: 0.7807\nNo improvement. Patience: 4/15\n--- Fold 3, Epoch 20/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss: 0.0443 | Val Loss: 0.0411 | Val R2: 0.7815\nNo improvement. Patience: 5/15\n--- Fold 3, Epoch 21/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss: 0.0442 | Val Loss: 0.0418 | Val R2: 0.8047\nNo improvement. Patience: 6/15\n--- Fold 3, Epoch 22/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss: 0.0477 | Val Loss: 0.0404 | Val R2: 0.8092\nNew best model for fold 3 saved with R2: 0.8092\n--- Fold 3, Epoch 23/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.04it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss: 0.0459 | Val Loss: 0.0396 | Val R2: 0.7733\nNo improvement. Patience: 1/15\n--- Fold 3, Epoch 24/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss: 0.0438 | Val Loss: 0.0409 | Val R2: 0.7976\nNo improvement. Patience: 2/15\n--- Fold 3, Epoch 25/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss: 0.0406 | Val Loss: 0.0417 | Val R2: 0.7572\nNo improvement. Patience: 3/15\n--- Fold 3, Epoch 26/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.04it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss: 0.0419 | Val Loss: 0.0377 | Val R2: 0.8091\nNo improvement. Patience: 4/15\n--- Fold 3, Epoch 27/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss: 0.0483 | Val Loss: 0.0389 | Val R2: 0.8082\nNo improvement. Patience: 5/15\n--- Fold 3, Epoch 28/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss: 0.0450 | Val Loss: 0.0359 | Val R2: 0.8172\nNew best model for fold 3 saved with R2: 0.8172\n--- Fold 3, Epoch 29/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss: 0.0445 | Val Loss: 0.0378 | Val R2: 0.8057\nNo improvement. Patience: 1/15\n--- Fold 3, Epoch 30/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss: 0.0411 | Val Loss: 0.0357 | Val R2: 0.8082\nNo improvement. Patience: 2/15\n--- Fold 3, Epoch 31/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.02it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss: 0.0410 | Val Loss: 0.0370 | Val R2: 0.8118\nNo improvement. Patience: 3/15\n--- Fold 3, Epoch 32/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.00it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss: 0.0438 | Val Loss: 0.0330 | Val R2: 0.8363\nNew best model for fold 3 saved with R2: 0.8363\n--- Fold 3, Epoch 33/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.00it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss: 0.0390 | Val Loss: 0.0383 | Val R2: 0.8162\nNo improvement. Patience: 1/15\n--- Fold 3, Epoch 34/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:09<00:00,  1.98it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss: 0.0449 | Val Loss: 0.0355 | Val R2: 0.8265\nNo improvement. Patience: 2/15\n--- Fold 3, Epoch 35/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss: 0.0399 | Val Loss: 0.0370 | Val R2: 0.7940\nNo improvement. Patience: 3/15\n--- Fold 3, Epoch 36/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss: 0.0392 | Val Loss: 0.0357 | Val R2: 0.8226\nNo improvement. Patience: 4/15\n--- Fold 3, Epoch 37/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss: 0.0384 | Val Loss: 0.0391 | Val R2: 0.8122\nNo improvement. Patience: 5/15\n--- Fold 3, Epoch 38/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss: 0.0401 | Val Loss: 0.0371 | Val R2: 0.8144\nNo improvement. Patience: 6/15\n--- Fold 3, Epoch 39/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss: 0.0436 | Val Loss: 0.0370 | Val R2: 0.8248\nNo improvement. Patience: 7/15\n--- Fold 3, Epoch 40/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.01it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss: 0.0445 | Val Loss: 0.0355 | Val R2: 0.8004\nNo improvement. Patience: 8/15\n--- Fold 3, Epoch 41/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss: 0.0406 | Val Loss: 0.0367 | Val R2: 0.8060\nNo improvement. Patience: 9/15\n--- Fold 3, Epoch 42/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss: 0.0395 | Val Loss: 0.0378 | Val R2: 0.8126\nNo improvement. Patience: 10/15\n--- Fold 3, Epoch 43/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.11it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss: 0.0413 | Val Loss: 0.0401 | Val R2: 0.7952\nNo improvement. Patience: 11/15\n--- Fold 3, Epoch 44/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.08it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss: 0.0412 | Val Loss: 0.0378 | Val R2: 0.8051\nNo improvement. Patience: 12/15\n--- Fold 3, Epoch 45/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss: 0.0425 | Val Loss: 0.0397 | Val R2: 0.8183\nNo improvement. Patience: 13/15\n--- Fold 3, Epoch 46/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss: 0.0422 | Val Loss: 0.0393 | Val R2: 0.8152\nNo improvement. Patience: 14/15\n--- Fold 3, Epoch 47/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss: 0.0404 | Val Loss: 0.0358 | Val R2: 0.8085\nNo improvement. Patience: 15/15\n--- Early stopping triggered at epoch 47 ---\nFold 3 complete. Best Validation R2: 0.8363\n=============================\n\n========== FOLD 4/5 ==========\n--- Fold 4, Epoch 1/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss: 0.9580 | Val Loss: 0.1941 | Val R2: -0.7668\nNew best model for fold 4 saved with R2: -0.7668\n--- Fold 4, Epoch 2/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.02it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss: 0.1714 | Val Loss: 0.0933 | Val R2: -0.1652\nNew best model for fold 4 saved with R2: -0.1652\n--- Fold 4, Epoch 3/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss: 0.1055 | Val Loss: 0.0590 | Val R2: 0.6179\nNew best model for fold 4 saved with R2: 0.6179\n--- Fold 4, Epoch 4/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss: 0.0971 | Val Loss: 0.0483 | Val R2: 0.7164\nNew best model for fold 4 saved with R2: 0.7164\n--- Fold 4, Epoch 5/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss: 0.0852 | Val Loss: 0.0604 | Val R2: 0.3255\nNo improvement. Patience: 1/15\n--- Fold 4, Epoch 6/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss: 0.0893 | Val Loss: 0.0498 | Val R2: 0.5470\nNo improvement. Patience: 2/15\n--- Fold 4, Epoch 7/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss: 0.0757 | Val Loss: 0.0448 | Val R2: 0.7437\nNew best model for fold 4 saved with R2: 0.7437\n--- Fold 4, Epoch 8/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss: 0.0739 | Val Loss: 0.0474 | Val R2: 0.6087\nNo improvement. Patience: 1/15\n--- Fold 4, Epoch 9/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.04it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss: 0.0655 | Val Loss: 0.0466 | Val R2: 0.5072\nNo improvement. Patience: 2/15\n--- Fold 4, Epoch 10/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.04it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss: 0.0714 | Val Loss: 0.0383 | Val R2: 0.7806\nNew best model for fold 4 saved with R2: 0.7806\n--- Fold 4, Epoch 11/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.01it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss: 0.0677 | Val Loss: 0.0470 | Val R2: 0.7167\nNo improvement. Patience: 1/15\n--- Fold 4, Epoch 12/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss: 0.0677 | Val Loss: 0.0729 | Val R2: 0.5059\nNo improvement. Patience: 2/15\n--- Fold 4, Epoch 13/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss: 0.0718 | Val Loss: 0.0538 | Val R2: 0.6037\nNo improvement. Patience: 3/15\n--- Fold 4, Epoch 14/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss: 0.0724 | Val Loss: 0.0682 | Val R2: 0.5649\nNo improvement. Patience: 4/15\n--- Fold 4, Epoch 15/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss: 0.0634 | Val Loss: 0.0550 | Val R2: 0.6472\nNo improvement. Patience: 5/15\n--- Fold 4, Epoch 16/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss: 0.0615 | Val Loss: 0.0582 | Val R2: 0.3902\nNo improvement. Patience: 6/15\n--- Fold 4, Epoch 17/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.08it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss: 0.0657 | Val Loss: 0.0712 | Val R2: 0.1096\nNo improvement. Patience: 7/15\n--- Fold 4, Epoch 18/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss: 0.0634 | Val Loss: 0.0500 | Val R2: 0.6308\nNo improvement. Patience: 8/15\n--- Fold 4, Epoch 19/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss: 0.0623 | Val Loss: 0.0357 | Val R2: 0.7294\nNo improvement. Patience: 9/15\n--- Fold 4, Epoch 20/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss: 0.0483 | Val Loss: 0.0359 | Val R2: 0.7589\nNo improvement. Patience: 10/15\n--- Fold 4, Epoch 21/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.04it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss: 0.0512 | Val Loss: 0.0350 | Val R2: 0.7614\nNo improvement. Patience: 11/15\n--- Fold 4, Epoch 22/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss: 0.0470 | Val Loss: 0.0371 | Val R2: 0.7576\nNo improvement. Patience: 12/15\n--- Fold 4, Epoch 23/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss: 0.0466 | Val Loss: 0.0337 | Val R2: 0.7975\nNew best model for fold 4 saved with R2: 0.7975\n--- Fold 4, Epoch 24/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss: 0.0412 | Val Loss: 0.0316 | Val R2: 0.7954\nNo improvement. Patience: 1/15\n--- Fold 4, Epoch 25/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss: 0.0405 | Val Loss: 0.0346 | Val R2: 0.7812\nNo improvement. Patience: 2/15\n--- Fold 4, Epoch 26/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.14it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss: 0.0422 | Val Loss: 0.0313 | Val R2: 0.7882\nNo improvement. Patience: 3/15\n--- Fold 4, Epoch 27/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.08it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss: 0.0436 | Val Loss: 0.0329 | Val R2: 0.7931\nNo improvement. Patience: 4/15\n--- Fold 4, Epoch 28/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss: 0.0454 | Val Loss: 0.0346 | Val R2: 0.7738\nNo improvement. Patience: 5/15\n--- Fold 4, Epoch 29/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss: 0.0452 | Val Loss: 0.0323 | Val R2: 0.7946\nNo improvement. Patience: 6/15\n--- Fold 4, Epoch 30/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss: 0.0487 | Val Loss: 0.0336 | Val R2: 0.8109\nNew best model for fold 4 saved with R2: 0.8109\n--- Fold 4, Epoch 31/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.11it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss: 0.0420 | Val Loss: 0.0356 | Val R2: 0.7757\nNo improvement. Patience: 1/15\n--- Fold 4, Epoch 32/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss: 0.0427 | Val Loss: 0.0373 | Val R2: 0.7615\nNo improvement. Patience: 2/15\n--- Fold 4, Epoch 33/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss: 0.0429 | Val Loss: 0.0322 | Val R2: 0.7822\nNo improvement. Patience: 3/15\n--- Fold 4, Epoch 34/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss: 0.0427 | Val Loss: 0.0313 | Val R2: 0.7894\nNo improvement. Patience: 4/15\n--- Fold 4, Epoch 35/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.08it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss: 0.0470 | Val Loss: 0.0337 | Val R2: 0.8203\nNew best model for fold 4 saved with R2: 0.8203\n--- Fold 4, Epoch 36/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss: 0.0405 | Val Loss: 0.0346 | Val R2: 0.7720\nNo improvement. Patience: 1/15\n--- Fold 4, Epoch 37/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.04it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss: 0.0384 | Val Loss: 0.0328 | Val R2: 0.8269\nNew best model for fold 4 saved with R2: 0.8269\n--- Fold 4, Epoch 38/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss: 0.0405 | Val Loss: 0.0320 | Val R2: 0.8049\nNo improvement. Patience: 1/15\n--- Fold 4, Epoch 39/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss: 0.0426 | Val Loss: 0.0308 | Val R2: 0.8269\nNo improvement. Patience: 2/15\n--- Fold 4, Epoch 40/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss: 0.0374 | Val Loss: 0.0355 | Val R2: 0.8158\nNo improvement. Patience: 3/15\n--- Fold 4, Epoch 41/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss: 0.0434 | Val Loss: 0.0326 | Val R2: 0.7886\nNo improvement. Patience: 4/15\n--- Fold 4, Epoch 42/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.15it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss: 0.0399 | Val Loss: 0.0351 | Val R2: 0.7944\nNo improvement. Patience: 5/15\n--- Fold 4, Epoch 43/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.16it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss: 0.0388 | Val Loss: 0.0344 | Val R2: 0.8095\nNo improvement. Patience: 6/15\n--- Fold 4, Epoch 44/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss: 0.0383 | Val Loss: 0.0338 | Val R2: 0.7656\nNo improvement. Patience: 7/15\n--- Fold 4, Epoch 45/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss: 0.0440 | Val Loss: 0.0375 | Val R2: 0.7729\nNo improvement. Patience: 8/15\n--- Fold 4, Epoch 46/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.11it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss: 0.0373 | Val Loss: 0.0347 | Val R2: 0.7826\nNo improvement. Patience: 9/15\n--- Fold 4, Epoch 47/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.14it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss: 0.0376 | Val Loss: 0.0345 | Val R2: 0.7725\nNo improvement. Patience: 10/15\n--- Fold 4, Epoch 48/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.11it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss: 0.0392 | Val Loss: 0.0336 | Val R2: 0.8003\nNo improvement. Patience: 11/15\n--- Fold 4, Epoch 49/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss: 0.0364 | Val Loss: 0.0329 | Val R2: 0.8009\nNo improvement. Patience: 12/15\n--- Fold 4, Epoch 50/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.14it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss: 0.0413 | Val Loss: 0.0316 | Val R2: 0.8246\nNo improvement. Patience: 13/15\n--- Fold 4, Epoch 51/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss: 0.0800 | Val Loss: 0.0682 | Val R2: 0.5793\nNew best model for fold 5 saved with R2: 0.5793\n--- Fold 5, Epoch 8/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss: 0.0856 | Val Loss: 0.0661 | Val R2: 0.6195\nNew best model for fold 5 saved with R2: 0.6195\n--- Fold 5, Epoch 9/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss: 0.0794 | Val Loss: 0.0780 | Val R2: 0.4398\nNo improvement. Patience: 1/15\n--- Fold 5, Epoch 10/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.09it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss: 0.0912 | Val Loss: 0.1133 | Val R2: 0.3155\nNo improvement. Patience: 2/15\n--- Fold 5, Epoch 11/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss: 0.0708 | Val Loss: 0.0604 | Val R2: 0.6947\nNew best model for fold 5 saved with R2: 0.6947\n--- Fold 5, Epoch 12/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.08it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss: 0.0630 | Val Loss: 0.0608 | Val R2: 0.6388\nNo improvement. Patience: 1/15\n--- Fold 5, Epoch 13/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.11it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss: 0.0696 | Val Loss: 0.0696 | Val R2: 0.6927\nNo improvement. Patience: 2/15\n--- Fold 5, Epoch 14/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss: 0.0690 | Val Loss: 0.0547 | Val R2: 0.5857\nNo improvement. Patience: 3/15\n--- Fold 5, Epoch 15/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.11it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss: 0.0583 | Val Loss: 0.0628 | Val R2: 0.5222\nNo improvement. Patience: 4/15\n--- Fold 5, Epoch 16/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss: 0.0606 | Val Loss: 0.0506 | Val R2: 0.7559\nNew best model for fold 5 saved with R2: 0.7559\n--- Fold 5, Epoch 17/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss: 0.0567 | Val Loss: 0.0512 | Val R2: 0.7057\nNo improvement. Patience: 1/15\n--- Fold 5, Epoch 18/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.14it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss: 0.0589 | Val Loss: 0.0682 | Val R2: 0.5526\nNo improvement. Patience: 2/15\n--- Fold 5, Epoch 19/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss: 0.0649 | Val Loss: 0.0512 | Val R2: 0.6704\nNo improvement. Patience: 3/15\n--- Fold 5, Epoch 20/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss: 0.0586 | Val Loss: 0.0532 | Val R2: 0.6441\nNo improvement. Patience: 4/15\n--- Fold 5, Epoch 21/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.08it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss: 0.0530 | Val Loss: 0.0513 | Val R2: 0.6549\nNo improvement. Patience: 5/15\n--- Fold 5, Epoch 22/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss: 0.0546 | Val Loss: 0.0563 | Val R2: 0.6984\nNo improvement. Patience: 6/15\n--- Fold 5, Epoch 23/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss: 0.0477 | Val Loss: 0.0570 | Val R2: 0.6801\nNo improvement. Patience: 7/15\n--- Fold 5, Epoch 24/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss: 0.0546 | Val Loss: 0.0457 | Val R2: 0.7119\nNo improvement. Patience: 8/15\n--- Fold 5, Epoch 25/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss: 0.0427 | Val Loss: 0.0383 | Val R2: 0.7582\nNew best model for fold 5 saved with R2: 0.7582\n--- Fold 5, Epoch 26/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss: 0.0395 | Val Loss: 0.0436 | Val R2: 0.7654\nNew best model for fold 5 saved with R2: 0.7654\n--- Fold 5, Epoch 27/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.08it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss: 0.0436 | Val Loss: 0.0483 | Val R2: 0.7019\nNo improvement. Patience: 1/15\n--- Fold 5, Epoch 28/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss: 0.0426 | Val Loss: 0.0479 | Val R2: 0.6952\nNo improvement. Patience: 2/15\n--- Fold 5, Epoch 29/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss: 0.0367 | Val Loss: 0.0453 | Val R2: 0.7539\nNo improvement. Patience: 3/15\n--- Fold 5, Epoch 30/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss: 0.0391 | Val Loss: 0.0409 | Val R2: 0.7401\nNo improvement. Patience: 4/15\n--- Fold 5, Epoch 31/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31: Train Loss: 0.0429 | Val Loss: 0.0442 | Val R2: 0.7191\nNo improvement. Patience: 5/15\n--- Fold 5, Epoch 32/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.11it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 32: Train Loss: 0.0408 | Val Loss: 0.0457 | Val R2: 0.7463\nNo improvement. Patience: 6/15\n--- Fold 5, Epoch 33/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.06it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 33: Train Loss: 0.0411 | Val Loss: 0.0436 | Val R2: 0.7571\nNo improvement. Patience: 7/15\n--- Fold 5, Epoch 34/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 34: Train Loss: 0.0421 | Val Loss: 0.0433 | Val R2: 0.7590\nNo improvement. Patience: 8/15\n--- Fold 5, Epoch 35/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.11it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 35: Train Loss: 0.0393 | Val Loss: 0.0432 | Val R2: 0.7493\nNo improvement. Patience: 9/15\n--- Fold 5, Epoch 36/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 36: Train Loss: 0.0373 | Val Loss: 0.0444 | Val R2: 0.7408\nNo improvement. Patience: 10/15\n--- Fold 5, Epoch 37/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 37: Train Loss: 0.0393 | Val Loss: 0.0406 | Val R2: 0.7504\nNo improvement. Patience: 11/15\n--- Fold 5, Epoch 38/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.01it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 38: Train Loss: 0.0355 | Val Loss: 0.0465 | Val R2: 0.7332\nNo improvement. Patience: 12/15\n--- Fold 5, Epoch 39/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.11it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 39: Train Loss: 0.0344 | Val Loss: 0.0464 | Val R2: 0.7103\nNo improvement. Patience: 13/15\n--- Fold 5, Epoch 40/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 40: Train Loss: 0.0382 | Val Loss: 0.0411 | Val R2: 0.7541\nNo improvement. Patience: 14/15\n--- Fold 5, Epoch 41/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 41: Train Loss: 0.0353 | Val Loss: 0.0366 | Val R2: 0.7827\nNew best model for fold 5 saved with R2: 0.7827\n--- Fold 5, Epoch 42/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.08it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 42: Train Loss: 0.0415 | Val Loss: 0.0455 | Val R2: 0.7308\nNo improvement. Patience: 1/15\n--- Fold 5, Epoch 43/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 43: Train Loss: 0.0407 | Val Loss: 0.0418 | Val R2: 0.7284\nNo improvement. Patience: 2/15\n--- Fold 5, Epoch 44/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.07it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 44: Train Loss: 0.0378 | Val Loss: 0.0454 | Val R2: 0.7721\nNo improvement. Patience: 3/15\n--- Fold 5, Epoch 45/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.12it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 45: Train Loss: 0.0441 | Val Loss: 0.0436 | Val R2: 0.7496\nNo improvement. Patience: 4/15\n--- Fold 5, Epoch 46/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.14it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 46: Train Loss: 0.0394 | Val Loss: 0.0421 | Val R2: 0.7382\nNo improvement. Patience: 5/15\n--- Fold 5, Epoch 47/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 47: Train Loss: 0.0407 | Val Loss: 0.0439 | Val R2: 0.7296\nNo improvement. Patience: 6/15\n--- Fold 5, Epoch 48/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.13it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 48: Train Loss: 0.0383 | Val Loss: 0.0430 | Val R2: 0.7370\nNo improvement. Patience: 7/15\n--- Fold 5, Epoch 49/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.10it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 49: Train Loss: 0.0415 | Val Loss: 0.0441 | Val R2: 0.7369\nNo improvement. Patience: 8/15\n--- Fold 5, Epoch 50/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.08it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 50: Train Loss: 0.0347 | Val Loss: 0.0522 | Val R2: 0.7243\nNo improvement. Patience: 9/15\n--- Fold 5, Epoch 51/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 51: Train Loss: 0.0384 | Val Loss: 0.0433 | Val R2: 0.7378\nNo improvement. Patience: 10/15\n--- Fold 5, Epoch 52/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 52: Train Loss: 0.0371 | Val Loss: 0.0415 | Val R2: 0.7412\nNo improvement. Patience: 11/15\n--- Fold 5, Epoch 53/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 53: Train Loss: 0.0373 | Val Loss: 0.0433 | Val R2: 0.7430\nNo improvement. Patience: 12/15\n--- Fold 5, Epoch 54/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.03it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 54: Train Loss: 0.0384 | Val Loss: 0.0421 | Val R2: 0.7708\nNo improvement. Patience: 13/15\n--- Fold 5, Epoch 55/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.05it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 55: Train Loss: 0.0381 | Val Loss: 0.0430 | Val R2: 0.7329\nNo improvement. Patience: 14/15\n--- Fold 5, Epoch 56/150 ---\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 18/18 [00:08<00:00,  2.11it/s]\nValidating: 100%|██████████| 5/5 [00:02<00:00,  2.13it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 56: Train Loss: 0.0381 | Val Loss: 0.0431 | Val R2: 0.7440\nNo improvement. Patience: 15/15\n--- Early stopping triggered at epoch 56 ---\nFold 5 complete. Best Validation R2: 0.7827\n=============================\n\n\n--- K-Fold Cross-Validation Complete ---\nR2 scores for each fold: [0.7860370874404907, 0.7061980366706848, 0.8362900614738464, 0.8269039988517761, 0.7826894521713257]\nAverage R2: 0.7876\nStd Dev R2: 0.0460\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":19}]}